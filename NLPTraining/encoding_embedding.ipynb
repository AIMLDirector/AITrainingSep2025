{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5285e4-f90f-49ad-8d72-54dad936df16",
   "metadata": {},
   "source": [
    "\n",
    "Embedding vectors and one-hot encoding are both techniques to represent categorical data numerically,\n",
    " but embeddings are learned, dense, and compact, capturing semantic relationships in a lower-dimensional \n",
    " space, while one-hot encoding creates sparse, high-dimensional vectors where each category is an orthogonal \n",
    " dimension, making embeddings better for large datasets and semantic tasks, and one-hot encoding suitable for \n",
    " small, fixed categories without inherent order.  \n",
    "\n",
    "\n",
    "One-Hot Encoding\n",
    "* How it works: Creates a binary vector for each category, where only one element (the \"hot\" one) is 1,\n",
    " and the rest are 0. \n",
    "* Pros: Simple, deterministic, and effective for nominal (no natural order) categorical variables. \n",
    "\n",
    "* Cons: Creates very high-dimensional and sparse vectors, which can lead to the \"curse of dimensionality\" and are inefficient for large datasets with many categories. Lacks semantic meaning and relationship between categories. \n",
    "\n",
    "Embeddings\n",
    "* How it works: Maps data into dense, lower-dimensional vectors where each dimension captures latent features or semantic meaning. \n",
    "* Pros: More computationally efficient, requires less memory, and can capture complex relationships and patterns, leading to better model performance and generalization. \n",
    "* Cons: Requires significant training data and computational resources. The quality depends on the underlying training algorithm (e.g., Word2Vec, GloVe). \n",
    "When to Use Which \n",
    "\n",
    "* Use One-Hot Encoding When: You have a small, fixed number of categories and don't need to capture semantic relationships between them. \n",
    "* Use Embeddings When: You have a large number of categories, or you need to leverage the semantic meaning and context of the data, as in natural language processing (NLP). \n",
    "\n",
    "Key Differences\n",
    "* Dimensionality: Embeddings are lower-dimensional and dense; one-hot encoding creates high-dimensional and sparse vectors. \n",
    "* Information Content: Embeddings encode semantic relationships; one-hot encoding provides no semantic information. \n",
    "* Learning: Embeddings are learned through machine learning models; one-hot encoding is a fixed, deterministic process. \n",
    "\n",
    "Word2Vec -- semantic relationship and meaning information \n",
    "* Developed by Google (Mikolov et al., 2013).\n",
    "* It converts words into dense vectors (called embeddings), where similar words are close to each other in vector space.\n",
    "* Example: king - man + woman ≈ queen.\n",
    " How it works:\n",
    "1. Two architectures:\n",
    "    * CBOW (Continuous Bag of Words): predicts a word from its surrounding words.\n",
    "    * Skip-Gram: predicts surrounding words given a word.\n",
    "2. The model trains on large text data and learns vector representations.\n",
    "3. After training, each word is represented as a fixed-size vector (e.g., 100-dimensional).\n",
    " Limitation: It only learns whole word embeddings, so unseen words (OOV – out of vocabulary) are not represented.\n",
    "\n",
    "i <love> eating   - CBOW  40 % \n",
    " I  love  eating icecream alone   -- skip - gram \n",
    " \n",
    "FastText\n",
    "* Developed by Facebook AI Research (2016).\n",
    "* It is an extension of Word2Vec.\n",
    "* Key difference: Instead of representing each word as a whole, FastText breaks words into subword units (character n-grams).\n",
    "Examples\n",
    "Word = “playing”\n",
    "* Subwords: “play”, “layi”, “ayin”, “ing”\n",
    "* Embedding is built from these pieces.  50 % \n",
    "Advantages of FastText:  -  \n",
    "* Handles rare words better.\n",
    "* Can create embeddings for out-of-vocabulary (OOV) words (e.g., misspellings, new words).\n",
    "* Especially useful for morphologically rich languages (like Hindi, Tamil, Turkish, Finnish, etc.).\n",
    "\n",
    "\n",
    "Word2Vec = word-level embeddings.\n",
    "FastText = word + subword embeddings (smarter for rare/OOV words).\n",
    "\n",
    "\n",
    "Feature\t   CBOW\t                            Skip-Gram\n",
    "Predicts\tWord from context\t          Context from word\n",
    "Speed\t     Faster\t                       Slower\n",
    "Accuracy\tGood for frequent words\t        Better for rare words\n",
    "Best for\tSmall datasets\t                   Large datasets\n",
    "\n",
    "\n",
    "2017 - transfomer ( word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530f96d8-8521-465e-86f6-c4603f83070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['bird', 'cat', 'dog', 'hat', 'in', 'mat', 'on', 'the', 'tree']\n",
      "Word to Index Mapping: {'bird': 0, 'cat': 1, 'dog': 2, 'hat': 3, 'in': 4, 'mat': 5, 'on': 6, 'the': 7, 'tree': 8}\n",
      "One-Hot Encoded Matrix:\n",
      "cat: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "in: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "hat: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "dog: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "on: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "mat: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "bird: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "in: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "the: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "tree: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(text):\n",
    "    words = text.split()\n",
    "    vocabulary = sorted(set(words))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    one_hot_encoded = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[word]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    return one_hot_encoded, word_to_index, vocabulary\n",
    "\n",
    "example_text = \"cat in the hat dog on the mat bird in the tree\"\n",
    "\n",
    "one_hot_encoded, word_to_index, vocabulary = one_hot_encode(example_text)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)                 # should work\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "for word, encoding in zip(example_text.split(), one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc999b26-668e-476c-b7f9-45eaa09cceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x1692e5a10>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = datapath('lee_background.cor')\n",
    "\n",
    "model = FastText(vector_size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c73d45-e931-4945-961f-3b0afc8a2bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x169851bd0>\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
    "    model.save(tmp.name, separately=[])\n",
    "\n",
    "# Load back the same model.\n",
    "loaded_model = FastText.load(tmp.name)\n",
    "print(loaded_model)\n",
    "\n",
    "os.unlink(tmp.name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0703554f-bda8-4dea-b464-8a8fcf4be488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x169a4dc50>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "wv = model.wv\n",
    "print(wv)\n",
    "\n",
    "#\n",
    "# FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word.\n",
    "#\n",
    "print('night' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402b8376-4efc-4b6d-856c-4a31a206339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('nights' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecab907d-3985-4dbe-a321-59c19b9d0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.20445213,  0.19205473, -0.2696594 , -0.08837921,  0.06665937,\n",
      "        0.37713125,  0.29362696,  0.4960144 ,  0.25551304, -0.2335439 ,\n",
      "        0.02672886, -0.16090696, -0.22832492,  0.510989  , -0.40147826,\n",
      "       -0.5596598 ,  0.18688893, -0.25138256, -0.42957833, -0.54374593,\n",
      "       -0.4709174 , -0.05898951, -0.45171773, -0.12916653, -0.20273305,\n",
      "       -0.3277675 , -0.6941753 , -0.1159768 , -0.33495227,  0.2764851 ,\n",
      "       -0.33012283,  0.30461985,  0.8438509 , -0.26730436,  0.18636127,\n",
      "        0.4047822 ,  0.3868081 , -0.1048426 , -0.3803687 , -0.34521735,\n",
      "        0.4718249 , -0.42686203,  0.02969932, -0.41831368, -0.5224052 ,\n",
      "       -0.304583  , -0.08432893,  0.11869861,  0.3761345 , -0.00140285,\n",
      "        0.35731208, -0.43189424,  0.296017  , -0.41103962, -0.1897895 ,\n",
      "       -0.18131128, -0.15968084, -0.13176629,  0.04600557, -0.35803026,\n",
      "       -0.3385793 , -0.44628662, -0.17897253,  0.34664026, -0.12272559,\n",
      "        0.69051284,  0.06279059,  0.06756178,  0.4308072 ,  0.24385183,\n",
      "       -0.23837976,  0.38786417,  0.5050551 , -0.6509997 ,  0.34773013,\n",
      "       -0.10376596,  0.2817812 , -0.04318111,  0.05804769,  0.38829395,\n",
      "        0.18295646, -0.49896604, -0.8083326 , -0.15793139, -0.12843491,\n",
      "       -0.7918183 ,  0.4865099 ,  0.18179044, -0.02441178, -0.27166933,\n",
      "       -0.00270576,  0.40360162, -0.11643653,  0.05552214, -0.19967818,\n",
      "        0.595827  , -0.221348  , -0.31888446, -0.02913882, -0.2249787 ],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(wv['night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd12f19-3d53-46a4-8030-02b9800eb4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-1.77679002e-01,  1.67121261e-01, -2.33521938e-01, -7.63154328e-02,\n",
      "        5.64601831e-02,  3.25218409e-01,  2.55535722e-01,  4.31267142e-01,\n",
      "        2.21683949e-01, -2.03850344e-01,  2.48830821e-02, -1.37788236e-01,\n",
      "       -1.98772654e-01,  4.40243781e-01, -3.48890901e-01, -4.85076696e-01,\n",
      "        1.61376417e-01, -2.17509687e-01, -3.71195912e-01, -4.72282678e-01,\n",
      "       -4.05491233e-01, -5.22639528e-02, -3.91675442e-01, -1.13440156e-01,\n",
      "       -1.74479946e-01, -2.82782376e-01, -6.00582361e-01, -9.83458012e-02,\n",
      "       -2.90395141e-01,  2.41430253e-01, -2.84523755e-01,  2.63830066e-01,\n",
      "        7.29589701e-01, -2.31260464e-01,  1.61623687e-01,  3.50224078e-01,\n",
      "        3.36478353e-01, -9.08171460e-02, -3.29726756e-01, -2.99643755e-01,\n",
      "        4.07969803e-01, -3.68960917e-01,  2.52312347e-02, -3.61906409e-01,\n",
      "       -4.53014016e-01, -2.62427658e-01, -7.00611174e-02,  1.03351928e-01,\n",
      "        3.26833248e-01, -1.33855865e-04,  3.10725361e-01, -3.73938143e-01,\n",
      "        2.56851554e-01, -3.55651498e-01, -1.63829759e-01, -1.55697063e-01,\n",
      "       -1.40239358e-01, -1.12333216e-01,  4.11644876e-02, -3.07409644e-01,\n",
      "       -2.92471647e-01, -3.87174398e-01, -1.54785678e-01,  2.99957931e-01,\n",
      "       -1.05949059e-01,  6.00316107e-01,  5.46202622e-02,  5.60034700e-02,\n",
      "        3.73487055e-01,  2.12720513e-01, -2.07265407e-01,  3.34567368e-01,\n",
      "        4.39387679e-01, -5.64766824e-01,  3.03240985e-01, -8.90106410e-02,\n",
      "        2.44127437e-01, -3.83355804e-02,  5.01857847e-02,  3.37299883e-01,\n",
      "        1.59315765e-01, -4.33459669e-01, -7.00921237e-01, -1.38135850e-01,\n",
      "       -1.10474437e-01, -6.88493669e-01,  4.22264069e-01,  1.57981873e-01,\n",
      "       -1.91401280e-02, -2.36394435e-01, -2.27365596e-03,  3.49054813e-01,\n",
      "       -1.01474285e-01,  4.88101430e-02, -1.73623741e-01,  5.16877711e-01,\n",
      "       -1.93376169e-01, -2.73072600e-01, -2.49615964e-02, -1.96088150e-01],\n",
      "      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(wv['nights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f40704c-4cf1-4c88-9bee-bd94197fcba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999166\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity(\"night\", \"nights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f75bd93-587d-4409-b92d-b01850b53732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('night', 0.999991774559021),\n",
      " ('flights', 0.9999871850013733),\n",
      " ('rights', 0.9999870657920837),\n",
      " ('overnight', 0.9999867677688599),\n",
      " ('fight', 0.999985933303833),\n",
      " ('fighting', 0.9999855160713196),\n",
      " ('entered', 0.9999850392341614),\n",
      " ('fighters', 0.9999849200248718),\n",
      " ('starting', 0.9999844431877136),\n",
      " ('fighter', 0.9999843835830688)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(\"nights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70bdfa-6819-4eaa-ad10-b273beb8a42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
