{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf787bb6-8473-45c0-b3b8-7fb8f478f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83e8c805-0185-4abf-a807-5351340fac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythonic\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d894d837-4091-44a8-88e3-04d8839ddbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "183166e6-5bd1-4de9-8069-342f94687b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"It is very important to be pythonly while you're pythoning with python. All pythoners should be pythonic. All pythoners have pythoned poorly atleast once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "368b13e1-ccfa-496c-b842-1bca32e27972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "'re\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "should\n",
      "be\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "atleast\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in word_tokenize(new_text):\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0534821b-491e-455b-b444-9c675d03db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "lanc = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "284a75ff-cbc6-4d53-bbd8-0a0779db1a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salti'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('cats')\n",
    "porter.stem('amazing')\n",
    "porter.stem('amazement')\n",
    "porter.stem('amaze')\n",
    "porter.stem('amazed')\n",
    "porter.stem('amazon')\n",
    "porter.stem('nation')\n",
    "porter.stem('premonition')\n",
    "\n",
    "# Comparison between Porter and Snowball\n",
    "porter.stem('loudly')\n",
    "snowball.stem('loudly')\n",
    "\n",
    "# Comparison between Snowball and Lancaster\n",
    "porter.stem('salty')\n",
    "snowball.stem('salty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f59665-d902-4844-96ad-d8ece2145483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "'re\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "should\n",
      "be\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "atleast\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# tokenization, stop word, stem \n",
    "\n",
    "for w in word_tokenize(new_text):\n",
    "    #tokens_without_sw = [word for word in w if not word in stop_words]\n",
    "    print(porter.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9ffa26-9bf4-4c0e-b288-0c5b85630de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'important', 'pythonly', \"'\", 'pythoning', 'python', '.', 'All', 'pythoners', 'pythonic', '.', 'All', 'pythoners', 'pythoned', 'poorly', 'atleast', '.']\n"
     ]
    }
   ],
   "source": [
    "words_filtered = []\n",
    "words = wordpunct_tokenize(new_text)\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        words_filtered.append(w)\n",
    "\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780cf444-adee-4126-8faf-b98cf83f5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "very\n",
      "import\n",
      "to\n",
      "be\n",
      "python\n",
      "whil\n",
      "you\n",
      "'re\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "al\n",
      "python\n",
      "should\n",
      "be\n",
      "python\n",
      ".\n",
      "al\n",
      "python\n",
      "hav\n",
      "python\n",
      "poor\n",
      "atleast\n",
      "ont\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in word_tokenize(new_text):\n",
    "    print(lanc.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c9195e-1306-46b5-8108-f8d3b9ff5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f6d778-4c42-4832-950d-aca14557f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"word\", \"wordy\", \"wording\", \"cacti\", \"rocks\", \"catty\", \"demonic\", \"geese\", \"ravishing\", \"better\", \"best\", \"run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b3d607-f1d0-4b0c-b8bf-eca45fdefc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "wordy\n",
      "wording\n",
      "cacti\n",
      "rocks\n",
      "catty\n",
      "demonic\n",
      "geese\n",
      "ravishing\n",
      "good\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w, pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499d09e0-ba8b-4eb7-8f79-4d8f9b6fcb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "wordy\n",
      "word\n",
      "cacti\n",
      "rock\n",
      "catty\n",
      "demonic\n",
      "geese\n",
      "ravish\n",
      "better\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60737134-defc-4344-a24d-39caded0b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "wordy\n",
      "wording\n",
      "cactus\n",
      "rock\n",
      "catty\n",
      "demonic\n",
      "goose\n",
      "ravishing\n",
      "better\n",
      "best\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "459fd4bc-9ce8-4365-999c-a954a9cad777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She ---> She\n",
      "jumped ---> jumped\n",
      "into ---> into\n",
      "the ---> the\n",
      "river ---> river\n",
      "and ---> and\n",
      "breathed ---> breathed\n",
      "heavily ---> heavily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"She jumped into the river and breathed heavily\"\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokenizer = word_tokenize(text)\n",
    "\n",
    "for token in tokenizer:\n",
    "    print(token,\"--->\",wordnet.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "279e9ce3-1f95-412e-b38a-17dc6d5c4c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ---> I\n",
      "am ---> am\n",
      "running ---> running\n",
      "and ---> and\n",
      "I ---> I\n",
      "usually ---> usually\n",
      "use ---> use\n",
      "to ---> to\n",
      "runs ---> run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I am running and I usually use to runs\"\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "tokenizer = word_tokenize(text)\n",
    "\n",
    "for token in tokenizer:\n",
    "    print(token,\"--->\",wordnet.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c0931b-6243-4093-99e1-b626c47fde4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She ---> She\n",
      "jumped ---> jump\n",
      "into ---> into\n",
      "the ---> the\n",
      "river ---> river\n",
      "and ---> and\n",
      "breathed ---> breathe\n",
      "heavily ---> heavily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "text = \"She jumped into the river and breathed heavily\"\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for token,tag in pos_tag(word_tokenize(text)):\n",
    "    pos=tag[0].lower()\n",
    "        \n",
    "    if pos not in ['a', 'r', 'n', 'v']:\n",
    "        pos='n'\n",
    "    \n",
    "    print(token,\"--->\",wordnet.lemmatize(token,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8768451-3a15-480b-a34a-a6b176c0d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ---> I\n",
      "am ---> be\n",
      "running ---> run\n",
      "and ---> and\n",
      "I ---> I\n",
      "usually ---> usually\n",
      "use ---> use\n",
      "to ---> to\n",
      "runs ---> run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "text = \"I am running and I usually use to runs\"\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "for token,tag in pos_tag(word_tokenize(text)):\n",
    "    pos=tag[0].lower()\n",
    "        \n",
    "    if pos not in ['a', 'r', 'n', 'v']:\n",
    "        pos='n'\n",
    "    \n",
    "    print(token,\"--->\",wordnet.lemmatize(token,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40b2546f-d64d-4449-9aca-ff6fcd78e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word      Snowball Stemmer    porter stemmer                Wordnet Lemmatizer                      \n",
      "better    better              better                        well                                    \n",
      "Caring    care                care                          Caring                                  \n",
      "are       are                 are                           be                                      \n",
      "am        am                  am                            be                                      \n",
      "worse     wors                wors                          worse                                   \n",
      "strugglingstruggl             struggl                       struggle                                \n",
      "meeting   meet                meet                          meeting                                 \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer,PorterStemmer,WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "snowball = SnowballStemmer(language='english')\n",
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "text = [\"better\",\"Caring\",\"are\",\"am\",\"worse\",\"struggling\",'meeting']\n",
    "print(\"{0:10}{1:20}{2:30}{3:40}\".format(\"Word\",\"Snowball Stemmer\",\"porter stemmer\", \"Wordnet Lemmatizer\",))\n",
    "for token,tag in pos_tag(text):\n",
    "    \n",
    "    pos=tag[0].lower()\n",
    "    if pos not in ['a', 'r', 'n', 'v']:\n",
    "        pos='n'\n",
    "        \n",
    "    print(\"{0:10}{1:20}{2:30}{3:40}\".format(token,snowball.stem(token),porter.stem(token),wordnet.lemmatize(token,pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d28e8f-6d0b-4325-bf7f-d56212e7361b",
   "metadata": {},
   "source": [
    "Tokenizing, stop word removal and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202bcf4-3572-49b4-9c35-5135636d2bc6",
   "metadata": {},
   "source": [
    "### We are going to do end to end  sentence pre-processing\n",
    "### Step 1  We need to do 2 time tokenization to remove the special characteres and tokenize cleanly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0894d42c-7012-475e-86c3-ebf04680039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'very', 'important', 'to', 'be', 'pythonly', 'while', 'you', \"'re\", 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'should', 'be', 'pythonic', '.', 'All', 'pythoners', 'have', 'pythoned', 'poorly', 'atleast', 'once', '.']\n",
      "It is very important to be pythonly while you re pythoning with python  All pythoners should be pythonic  All pythoners have pythoned poorly atleast once \n"
     ]
    }
   ],
   "source": [
    "import re, nltk\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "print(tokens)\n",
    "# Remove special characters from each token\n",
    "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
    "    \n",
    "    # If you want to remove leading/trailing whitespaces and combine them into a string\n",
    "cleaned_text = ' '.join(cleaned_tokens)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23af80-72a6-4438-b603-c52d389543c6",
   "metadata": {},
   "source": [
    "### Step 2 : We need to do remove the stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e2fa75c-63f6-492c-9598-07978d9fa10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'important', 'pythonly', 'pythoning', 'python', 'All', 'pythoners', 'pythonic', 'All', 'pythoners', 'pythoned', 'poorly', 'atleast']\n"
     ]
    }
   ],
   "source": [
    "words_filtered = []\n",
    "words = wordpunct_tokenize(cleaned_text)\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        words_filtered.append(w)\n",
    "\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f3cd2-8a73-4cf1-8379-b62b3f16b99f",
   "metadata": {},
   "source": [
    "### Step 3 :  part of speech tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c58ad59e-b242-413f-872a-e4eee53e2463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It | PRP\n",
      "important | JJ\n",
      "pythonly | RB\n",
      "pythoning | VBG\n",
      "python | IN\n",
      "All | NNP\n",
      "pythoners | NNS\n",
      "pythonic | VBP\n",
      "All | DT\n",
      "pythoners | NNS\n",
      "pythoned | VBD\n",
      "poorly | RB\n",
      "atleast | JJ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "for word, pos in pos_tag(words_filtered):\n",
    "    print(word, '|', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fcb5c-dd76-412c-b07b-1d072c6c32d6",
   "metadata": {},
   "source": [
    "### Step 4 : lemmatizing the word and check whether we are able to prepare the root word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c918f066-2d42-430f-afe3-89cb5b155a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It ---> It\n",
      "important ---> important\n",
      "pythonly ---> pythonly\n",
      "pythoning ---> pythoning\n",
      "python ---> python\n",
      "All ---> All\n",
      "pythoners ---> pythoners\n",
      "pythonic ---> pythonic\n",
      "All ---> All\n",
      "pythoners ---> pythoners\n",
      "pythoned ---> pythoned\n",
      "poorly ---> poorly\n",
      "atleast ---> atleast\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token,tag in pos_tag(words_filtered):\n",
    "    pos=tag[0].lower()\n",
    "        \n",
    "    if pos not in ['a', 'r', 'n', 'v']:\n",
    "        pos='n'\n",
    "    \n",
    "    print(token,\"--->\",wordnet.lemmatize(token,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c04568-97a7-40d6-8b23-ee08aa008fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "lanc = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c50f66fd-bded-4773-826b-33eaca8264ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_definition = '''\n",
    "The European Union (EU) is a political and economic union of 27 member states that are located primarily in Europe. \n",
    "Its members have a combined area of 4,233,255.3 km2 (1,634,469.0 sq mi) and an estimated total population of about 447 million. \n",
    "The EU has developed an internal single market through a standardised system of laws that apply in all member states in those matters, \n",
    "and only those matters, where members have agreed to act as one. EU policies aim to ensure the free movement of people, goods, \n",
    "services and capital within the internal market; enact legislation in justice and home affairs; and maintain common policies on trade, \n",
    "agriculture, fisheries and regional development. Passport controls have been abolished for travel within the Schengen Area. \n",
    "A monetary union was established in 1999, coming into full force in 2002, and is composed of 19 EU member states which use the euro \n",
    "currency. The EU has often been described as a sui generis political entity (without precedent or comparison).\n",
    "The EU and European citizenship were established when the Maastricht Treaty came into force in 1993. \n",
    "The EU traces its origins to the European Coal and Steel Community (ECSC) and the European Economic Community (EEC), established, \n",
    "respectively, by the 1951 Treaty of Paris and 1957 Treaty of Rome. The original members of what came to be known as the European \n",
    "Communities were the Inner Six: Belgium, France, Italy, Luxembourg, the Netherlands, and West Germany. The Communities and their \n",
    "successors have grown in size by the accession of new member states and in power by the addition of policy areas to their remit. \n",
    "The United Kingdom became the first member state to leave the EU on 31 January 2020. Before this, three territories of member states \n",
    "had left the EU or its forerunners. The latest major amendment to the constitutional basis of the EU, the Treaty of Lisbon, \n",
    "came into force in 2009.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49a23fb4-ed3b-4524-b948-974782753656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'definit', 'a', 'controversi', 'as', 'the', 'attorney', 'label', 'the', 'case', '``', 'extrem', 'controversi', \"''\"]\n",
      "['the', 'european', 'union', '(', 'eu', ')', 'is', 'a', 'polit', 'and', 'econom', 'union', 'of', '27', 'member', 'state', 'that', 'are', 'locat', 'primarili', 'in', 'europ', '.', 'it', 'member', 'have', 'a', 'combin', 'area', 'of', '4,233,255.3', 'km2', '(', '1,634,469.0', 'sq', 'mi', ')', 'and', 'an', 'estim', 'total', 'popul', 'of', 'about', '447', 'million', '.', 'the', 'eu', 'ha', 'develop', 'an', 'intern', 'singl', 'market', 'through', 'a', 'standardis', 'system', 'of', 'law', 'that', 'appli', 'in', 'all', 'member', 'state', 'in', 'those', 'matter', ',', 'and', 'onli', 'those', 'matter', ',', 'where', 'member', 'have', 'agre', 'to', 'act', 'as', 'one', '.', 'eu', 'polici', 'aim', 'to', 'ensur', 'the', 'free', 'movement', 'of', 'peopl', ',', 'good', ',', 'servic', 'and', 'capit', 'within', 'the', 'intern', 'market', ';', 'enact', 'legisl', 'in', 'justic', 'and', 'home', 'affair', ';', 'and', 'maintain', 'common', 'polici', 'on', 'trade', ',', 'agricultur', ',', 'fisheri', 'and', 'region', 'develop', '.', 'passport', 'control', 'have', 'been', 'abolish', 'for', 'travel', 'within', 'the', 'schengen', 'area', '.', 'a', 'monetari', 'union', 'wa', 'establish', 'in', '1999', ',', 'come', 'into', 'full', 'forc', 'in', '2002', ',', 'and', 'is', 'compos', 'of', '19', 'eu', 'member', 'state', 'which', 'use', 'the', 'euro', 'currenc', '.', 'the', 'eu', 'ha', 'often', 'been', 'describ', 'as', 'a', 'sui', 'generi', 'polit', 'entiti', '(', 'without', 'preced', 'or', 'comparison', ')', '.', 'the', 'eu', 'and', 'european', 'citizenship', 'were', 'establish', 'when', 'the', 'maastricht', 'treati', 'came', 'into', 'forc', 'in', '1993', '.', 'the', 'eu', 'trace', 'it', 'origin', 'to', 'the', 'european', 'coal', 'and', 'steel', 'commun', '(', 'ecsc', ')', 'and', 'the', 'european', 'econom', 'commun', '(', 'eec', ')', ',', 'establish', ',', 'respect', ',', 'by', 'the', '1951', 'treati', 'of', 'pari', 'and', '1957', 'treati', 'of', 'rome', '.', 'the', 'origin', 'member', 'of', 'what', 'came', 'to', 'be', 'known', 'as', 'the', 'european', 'commun', 'were', 'the', 'inner', 'six', ':', 'belgium', ',', 'franc', ',', 'itali', ',', 'luxembourg', ',', 'the', 'netherland', ',', 'and', 'west', 'germani', '.', 'the', 'commun', 'and', 'their', 'successor', 'have', 'grown', 'in', 'size', 'by', 'the', 'access', 'of', 'new', 'member', 'state', 'and', 'in', 'power', 'by', 'the', 'addit', 'of', 'polici', 'area', 'to', 'their', 'remit', '.', 'the', 'unit', 'kingdom', 'becam', 'the', 'first', 'member', 'state', 'to', 'leav', 'the', 'eu', 'on', '31', 'januari', '2020', '.', 'befor', 'thi', ',', 'three', 'territori', 'of', 'member', 'state', 'had', 'left', 'the', 'eu', 'or', 'it', 'forerunn', '.', 'the', 'latest', 'major', 'amend', 'to', 'the', 'constitut', 'basi', 'of', 'the', 'eu', ',', 'the', 'treati', 'of', 'lisbon', ',', 'came', 'into', 'forc', 'in', '2009', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_example = (\n",
    "  'This is definitely a controversy as the attorney labeled the case \"extremely controversial\"'\n",
    ")\n",
    "\n",
    "# Porter Stemmed version of sentence example\n",
    "stemmed_sentence = [\n",
    "  porter.stem(word) for word in word_tokenize(sentence_example)\n",
    "]\n",
    "print(stemmed_sentence)\n",
    "\n",
    "\n",
    "# Tokenizing and Stemming the eu_definition\n",
    "tokenized_eu = word_tokenize(eu_definition)\n",
    "porter_eu = [porter.stem(word) for word in tokenized_eu]\n",
    "print(porter_eu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0e9e49-a2ec-4d2f-a98d-e1029d8cc1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
