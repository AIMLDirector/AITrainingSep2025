{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NLTK- Natural Language Toolkit </h1>\r\n",
    "\r\n",
    "<h2>NLTK: A tool for NLP\r\n",
    "NLP: Process of converting the natural languages to computer understandable data.\r\n",
    "\r\n",
    "NLTK documentation link: https://www.nltk.org/\r\n",
    "\r\n",
    "*To install nltk:*\r\n",
    "pip3 install nltk\r\n",
    "\r\n",
    "import nltk and download the nltk packages</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the popup opens, select \"all\" packages and download. After downloading finishes, click \"finish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenising: It's a form of grouping things\n",
    "Basic types of tokenisers: Word Tokeniser and Sentence Tokeniser\n",
    "\n",
    "Lexicons: words and their meanings - gets tricky with semantic analysis\n",
    "\n",
    "Corporas: body of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"First Sentence. Second Sentence. Mr. Three's Sentence. Who's Mr. Four?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text1 = \"Langfuse is one of the original tools in the LLM observability space. This means it has a wide range of tools for LLM app developers to use and have been instrumental in defining what they look like.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Langfuse is one of the original tools in the LLM observability space.', 'This means it has a wide range of tools for LLM app developers to use and have been instrumental in defining what they look like.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Langfuse', 'is', 'one', 'of', 'the', 'original', 'tools', 'in', 'the', 'LLM', 'observability', 'space', '.', 'This', 'means', 'it', 'has', 'a', 'wide', 'range', 'of', 'tools', 'for', 'LLM', 'app', 'developers', 'to', 'use', 'and', 'have', 'been', 'instrumental', 'in', 'defining', 'what', 'they', 'look', 'like', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "original\n",
      "tools\n",
      "in\n",
      "the\n",
      "LLM\n",
      "observability\n",
      "space\n",
      ".\n",
      "This\n",
      "means\n",
      "it\n",
      "has\n",
      "a\n",
      "wide\n",
      "range\n",
      "of\n",
      "tools\n",
      "for\n",
      "LLM\n",
      "app\n",
      "developers\n",
      "to\n",
      "use\n",
      "and\n",
      "have\n",
      "been\n",
      "instrumental\n",
      "in\n",
      "defining\n",
      "what\n",
      "they\n",
      "look\n",
      "like\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for x in word_tokenize(example_text1): print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence', '.', 'Second', 'Sentence', '.', 'Mr', '.', 'Three', \"'\", 's', 'Sentence', '.', 'Who', \"'\", 's', 'Mr', '.', 'Four', '?']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence.', 'Second', 'Sentence.', 'Mr.', 'Three', \"'s\", 'Sentence.', 'Who', \"'s\", 'Mr.', 'Four', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- --------------\n",
      "absl-py                                  2.2.2\n",
      "aiohappyeyeballs                         2.6.1\n",
      "aiohttp                                  3.12.9\n",
      "aiosignal                                1.3.2\n",
      "annotated-types                          0.7.0\n",
      "anyio                                    4.9.0\n",
      "appnope                                  0.1.4\n",
      "arrow                                    1.3.0\n",
      "asttokens                                3.0.0\n",
      "astunparse                               1.6.3\n",
      "attrs                                    25.3.0\n",
      "azure-ai-inference                       1.0.0b9\n",
      "azure-core                               1.34.0\n",
      "azure-identity                           1.23.0\n",
      "blis                                     1.2.1\n",
      "catalogue                                2.0.10\n",
      "certifi                                  2025.4.26\n",
      "cffi                                     1.17.1\n",
      "charset-normalizer                       3.4.2\n",
      "click                                    8.2.0\n",
      "cloudpathlib                             0.21.0\n",
      "codecov                                  2.1.13\n",
      "colorama                                 0.4.6\n",
      "comm                                     0.2.2\n",
      "confection                               0.1.5\n",
      "coverage                                 7.8.2\n",
      "cryptography                             45.0.3\n",
      "curated-tokenizers                       0.0.9\n",
      "curated-transformers                     0.1.1\n",
      "cymem                                    2.0.11\n",
      "debugpy                                  1.8.14\n",
      "decorator                                5.2.1\n",
      "detoxify                                 0.5.2\n",
      "diff-match-patch                         20230430\n",
      "distlib                                  0.3.9\n",
      "distro                                   1.9.0\n",
      "en_core_web_sm                           3.8.0\n",
      "en_core_web_trf                          3.8.0\n",
      "executing                                2.2.0\n",
      "Faker                                    25.9.2\n",
      "filelock                                 3.18.0\n",
      "flatbuffers                              25.2.10\n",
      "fqdn                                     1.5.1\n",
      "frozenlist                               1.6.2\n",
      "fsspec                                   2025.3.2\n",
      "gast                                     0.6.0\n",
      "gensim                                   4.3.3\n",
      "google-pasta                             0.2.0\n",
      "googleapis-common-protos                 1.70.0\n",
      "griffe                                   0.36.9\n",
      "grpcio                                   1.71.0\n",
      "guardrails-ai                            0.6.6\n",
      "guardrails-api-client                    0.4.0a1\n",
      "guardrails-grhub-competitor-check        0.0.1\n",
      "guardrails-grhub-llamaguard-7b           0.0.1\n",
      "guardrails-grhub-toxic-language          0.0.2\n",
      "guardrails_hub_types                     0.0.4\n",
      "h11                                      0.16.0\n",
      "h5py                                     3.13.0\n",
      "httpcore                                 1.0.9\n",
      "httpx                                    0.28.1\n",
      "huggingface-hub                          0.31.4\n",
      "idna                                     3.10\n",
      "importlib_metadata                       8.7.0\n",
      "ipykernel                                6.29.5\n",
      "ipython                                  9.2.0\n",
      "ipython_pygments_lexers                  1.1.1\n",
      "isodate                                  0.7.2\n",
      "isoduration                              20.11.0\n",
      "jedi                                     0.19.2\n",
      "Jinja2                                   3.1.6\n",
      "jiter                                    0.10.0\n",
      "joblib                                   1.5.0\n",
      "jsonpatch                                1.33\n",
      "jsonpointer                              3.0.0\n",
      "jsonref                                  1.1.0\n",
      "jsonschema                               4.24.0\n",
      "jsonschema-specifications                2025.4.1\n",
      "jupyter_client                           8.6.3\n",
      "jupyter_core                             5.7.2\n",
      "keras                                    3.10.0\n",
      "langchain-core                           0.3.63\n",
      "langcodes                                3.5.0\n",
      "langsmith                                0.3.45\n",
      "language_data                            1.3.0\n",
      "libclang                                 18.1.1\n",
      "litellm                                  1.72.1\n",
      "lxml                                     4.9.4\n",
      "marisa-trie                              1.2.1\n",
      "Markdown                                 3.8\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               3.0.2\n",
      "matplotlib-inline                        0.1.7\n",
      "mdurl                                    0.1.2\n",
      "ml_dtypes                                0.5.1\n",
      "mpmath                                   1.3.0\n",
      "msal                                     1.32.3\n",
      "msal-extensions                          1.3.1\n",
      "multidict                                6.4.4\n",
      "murmurhash                               1.0.12\n",
      "namex                                    0.0.9\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.4.2\n",
      "nltk                                     3.9.1\n",
      "numpy                                    1.26.4\n",
      "openai                                   1.84.0\n",
      "opentelemetry-api                        1.34.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.34.0\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.34.0\n",
      "opentelemetry-exporter-otlp-proto-http   1.34.0\n",
      "opentelemetry-proto                      1.34.0\n",
      "opentelemetry-sdk                        1.34.0\n",
      "opentelemetry-semantic-conventions       0.55b0\n",
      "opt_einsum                               3.4.0\n",
      "optree                                   0.15.0\n",
      "orjson                                   3.10.18\n",
      "packaging                                24.2\n",
      "pandas                                   2.2.3\n",
      "parso                                    0.8.4\n",
      "pexpect                                  4.9.0\n",
      "pip                                      23.2.1\n",
      "platformdirs                             4.3.8\n",
      "preshed                                  3.0.9\n",
      "prompt_toolkit                           3.0.51\n",
      "propcache                                0.3.1\n",
      "protobuf                                 5.29.4\n",
      "psutil                                   7.0.0\n",
      "ptyprocess                               0.7.0\n",
      "pure_eval                                0.2.3\n",
      "py4j                                     0.10.9.9\n",
      "pycparser                                2.22\n",
      "pydantic                                 2.11.4\n",
      "pydantic_core                            2.33.2\n",
      "pydash                                   7.0.7\n",
      "Pygments                                 2.19.1\n",
      "PyJWT                                    2.10.1\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.1.0\n",
      "pytz                                     2025.2\n",
      "PyYAML                                   6.0.2\n",
      "pyzmq                                    26.4.0\n",
      "referencing                              0.36.2\n",
      "regex                                    2024.11.6\n",
      "requests                                 2.32.3\n",
      "requests-toolbelt                        1.0.0\n",
      "rfc3339-validator                        0.1.4\n",
      "rfc3986-validator                        0.1.1\n",
      "rich                                     13.9.4\n",
      "rpds-py                                  0.25.1\n",
      "rstr                                     3.2.2\n",
      "safetensors                              0.5.3\n",
      "scipy                                    1.13.1\n",
      "semver                                   3.0.4\n",
      "sentencepiece                            0.2.0\n",
      "setuptools                               65.5.0\n",
      "shellingham                              1.5.4\n",
      "six                                      1.17.0\n",
      "smart-open                               7.1.0\n",
      "sniffio                                  1.3.1\n",
      "spacy                                    3.8.5\n",
      "spacy-curated-transformers               0.3.1\n",
      "spacy-legacy                             3.0.12\n",
      "spacy-loggers                            1.0.5\n",
      "srsly                                    2.5.1\n",
      "stack-data                               0.6.3\n",
      "sympy                                    1.14.0\n",
      "tenacity                                 9.1.2\n",
      "tensorboard                              2.19.0\n",
      "tensorboard-data-server                  0.7.2\n",
      "tensorflow                               2.19.0\n",
      "tensorflow-io-gcs-filesystem             0.37.1\n",
      "termcolor                                3.1.0\n",
      "tf_keras                                 2.19.0\n",
      "thinc                                    8.3.4\n",
      "tiktoken                                 0.9.0\n",
      "tokenizers                               0.21.1\n",
      "torch                                    2.7.0\n",
      "tornado                                  6.4.2\n",
      "tqdm                                     4.67.1\n",
      "traitlets                                5.14.3\n",
      "transformers                             4.51.3\n",
      "typer                                    0.15.3\n",
      "types-python-dateutil                    2.9.0.20250516\n",
      "typing_extensions                        4.13.2\n",
      "typing-inspection                        0.4.0\n",
      "tzdata                                   2025.2\n",
      "uri-template                             1.3.0\n",
      "urllib3                                  2.0.7\n",
      "virtualenv                               20.31.2\n",
      "wasabi                                   1.1.3\n",
      "wcwidth                                  0.2.13\n",
      "weasel                                   0.4.1\n",
      "webcolors                                24.11.1\n",
      "Werkzeug                                 3.1.3\n",
      "wheel                                    0.45.1\n",
      "wrapt                                    1.17.2\n",
      "yarl                                     1.20.0\n",
      "zipp                                     3.22.0\n",
      "zstandard                                0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'her', 'during', 'you', 'themselves', 'y', 'itself', 'having', \"he'll\", 'should', 'theirs', \"they've\", 'himself', 'at', 'been', 'than', 'up', 'mustn', 'we', \"you'd\", 'shan', 'hadn', 'are', \"it's\", 'my', 'am', 'a', \"we've\", \"isn't\", 'and', 'off', 'were', 'not', 'or', \"should've\", \"she'll\", 'too', \"it'd\", 'couldn', 'most', \"they're\", \"we'll\", 'the', 'ma', 'no', 'because', 'all', 'again', 'ain', \"wouldn't\", 'had', \"needn't\", 'shouldn', 'under', 'for', 't', \"doesn't\", 'with', 'ourselves', 'an', 'of', \"we'd\", 'but', \"they'll\", 'mightn', 'through', \"i'll\", 'until', 'do', 'o', 'into', 'whom', 'does', 'more', 'above', \"didn't\", 'nor', 'yourself', 'about', 'here', \"weren't\", 'what', 'as', 'just', \"won't\", 'she', 'against', 'hers', \"i'd\", 'haven', \"i've\", \"mightn't\", 'wasn', 'he', 'them', 'yourselves', 'when', \"i'm\", 'doing', 'some', 'then', 'your', 'other', 'didn', \"we're\", 'myself', \"don't\", \"she'd\", 'hasn', 're', 'don', 'down', 'so', \"that'll\", 'very', 'only', 'their', 'to', 'herself', 'both', 'won', \"shouldn't\", \"hasn't\", 'which', \"mustn't\", 'those', 'out', 'any', 'can', 'ours', 'each', 'further', 'is', \"you've\", 's', 'his', 'this', 'where', 'below', 'll', \"they'd\", \"aren't\", 'being', 'if', \"he's\", 'between', \"couldn't\", 'will', 'have', 'from', 'on', \"shan't\", 'doesn', 'him', 'yours', 'did', \"hadn't\", 'its', 've', \"you're\", 'while', 'how', 'now', 'me', 'aren', 'was', \"he'd\", 'there', 'd', \"she's\", 'why', \"wasn't\", 'that', 'needn', 'has', 'wouldn', 'isn', 'before', 'once', 'in', \"haven't\", 'after', 'own', \"you'll\", 'be', 'm', 'over', 'weren', 'by', 'i', \"it'll\", 'our', 'these', 'they', 'few', 'it', 'who', 'same', 'such'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'is', 'an', 'American', 'online', 'social', 'media', 'and', 'social', 'networking', 'service', 'owned', 'by', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('online')\n",
    "\n",
    "\n",
    "text_tokens = word_tokenize(example_sentence)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'media', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "sw_list = ['social','online']\n",
    "all_stopwords.extend(sw_list)\n",
    "\n",
    "text_tokens = word_tokenize(example_sentence)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/58/6a/9166369a2f092bd286d24e6307de555d63616e8ddb373ebad2b5635ca4cd/ipywidgets-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipywidgets) (9.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.14 from https://files.pythonhosted.org/packages/ca/51/5447876806d1088a0f8f71e16542bf350918128d0a69437df26047c8e46f/widgetsnbextension-4.0.14-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/jupyterlab-widgets/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab_widgets~=3.0.15 from https://files.pythonhosted.org/packages/43/6a/ca128561b22b60bd5a0c4ea26649e68c8556b82bc70a0c396eebc977fe86/jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords.add(\"Inc\")\n",
    "\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy                                    1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list|grep -i numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "135e78ef6267b613ce7b86630936d470174b66187aad9f784a45e5cc3235687c"
  },
  "kernelspec": {
   "display_name": "AILearningSep",
   "language": "python",
   "name": "ailearningsep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
