{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat are Stop words?\\n\\nStop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore,\\nboth when indexing entries for searching and when retrieving them as the result of a search query. \\nWe would not want these words to take up space in our database, or taking up valuable processing time.\\nFor this, we can remove them easily, by storing a list of words that you consider to stop words. \\nNLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\\nYou can find them in the nltk_data directory.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\r\n",
    "What are Stop words?\r\n",
    "\r\n",
    "Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore,\r\n",
    "both when indexing entries for searching and when retrieving them as the result of a search query. \r\n",
    "We would not want these words to take up space in our database, or taking up valuable processing time.\r\n",
    "For this, we can remove them easily, by storing a list of words that you consider to stop words. \r\n",
    "NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\r\n",
    "You can find them in the nltk_data directory.\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'their', 'hadn', 'which', \"we'll\", 'where', 'under', 'am', 't', 'y', \"we'd\", 'in', \"i'd\", \"they're\", 'theirs', 'they', \"he'll\", \"should've\", \"mightn't\", 'were', 'will', 'isn', 'his', 'with', \"doesn't\", 'until', 'few', 'it', 'so', 'once', 'now', \"we've\", 'we', 'most', 'who', \"it's\", 'did', 'if', 'hers', 'of', 'out', 'hasn', 'very', 'as', \"hasn't\", \"it'll\", 'her', \"i'm\", 'should', 'herself', 'for', \"she's\", 'having', 've', 'ma', \"won't\", \"isn't\", 'don', \"they'd\", \"it'd\", 'off', 'me', \"haven't\", 'through', 'from', \"she'd\", \"shouldn't\", 'mustn', 'whom', 'i', 'mightn', 'd', 'couldn', \"wouldn't\", 'ourselves', 'just', 'some', 'there', 'the', \"don't\", 'my', 'shan', 'your', 'wasn', 'up', \"he'd\", 'more', 'himself', 'needn', 'all', 'what', 'shouldn', 'these', \"you've\", 'then', 'while', 'you', 'do', 'own', 'again', 'into', 'here', 'this', 'them', 'll', \"didn't\", 'ours', 'him', 'but', 'such', \"they'll\", 'too', 'be', \"i'll\", 'during', 'than', 'yourself', 'by', 'is', 'about', \"shan't\", 'itself', 'yours', 'ain', 'can', \"aren't\", 'or', 'she', \"she'll\", \"couldn't\", 'not', 'm', \"hadn't\", \"needn't\", 'had', 'other', 's', \"you'll\", \"they've\", 'both', 'only', 'against', 'below', 'before', 'nor', 'have', \"mustn't\", \"wasn't\", 'are', \"he's\", 'how', 'yourselves', 'on', 'down', 'its', 'being', 'doing', 'any', 'themselves', 'to', \"i've\", 'between', 'above', 'over', \"you'd\", 'those', 'an', 'weren', \"that'll\", 'and', 'myself', 'has', 'o', 'our', 'each', 're', 'after', 'at', 'that', 'when', \"weren't\", 'a', 'he', 'same', 'wouldn', 'further', 'why', 'been', 'didn', 'aren', \"we're\", 'won', 'doesn', 'because', 'does', 'haven', 'no', \"you're\", 'was'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'is', 'an', 'American', 'online', 'social', 'media', 'and', 'social', 'networking', 'service', 'owned', 'by', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = []\r\n",
    "filtered_sentence = [w for w in words if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'----Additional Things----'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''----Additional Things----'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" READ:\\nTo add a word to NLTK stop words collection, first create an object from the stopwords.words('english') list. \\nNext, use the append() method on the list to add any word to the list.\\nThe following script adds the word play to the NLTK stop word collection.\\nAgain, we remove all the words from our text variable to see if the word play is removed or not.\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding Stop Words to Default NLTK Stop Word List\r\n",
    "''' READ:\r\n",
    "To add a word to NLTK stop words collection, first create an object from the stopwords.words('english') list. \r\n",
    "Next, use the append() method on the list to add any word to the list.\r\n",
    "The following script adds the word play to the NLTK stop word collection.\r\n",
    "Again, we remove all the words from our text variable to see if the word play is removed or not.\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = stopwords.words('english')\r\n",
    "all_stopwords.append('online')\r\n",
    "\r\n",
    "text_tokens = word_tokenize(example_sentence)\r\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output shows that the word 'online' has been removed.\r\n",
    "\r\n",
    "#You can also add a list of words to the stopwords.words list using the append method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'media', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "sw_list = ['social','online']\r\n",
    "all_stopwords.extend(sw_list)\r\n",
    "\r\n",
    "text_tokens = word_tokenize(example_sentence)\r\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSince stopwords.word(\\'english\\') is merely a list of items, you can remove items from this list like any other list.\\nThe simplest way to do so is via the remove() method. \\nThis is helpful for when your application needs a stop word to not be removed.\\nFor example, you may need to keep the word \"is\" in a sentence to know when a statement is being negated.\\n\\nThe following script removes the stop word  \"is\"  from the default list of stop words in NLTK:\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Removing Stop Words from Default NLTK Stop Word List\r\n",
    "\r\n",
    "'''\r\n",
    "Since stopwords.word('english') is merely a list of items, you can remove items from this list like any other list.\r\n",
    "The simplest way to do so is via the remove() method. \r\n",
    "This is helpful for when your application needs a stop word to not be removed.\r\n",
    "For example, you may need to keep the word \"is\" in a sentence to know when a statement is being negated.\r\n",
    "\r\n",
    "The following script removes the stop word  \"is\"  from the default list of stop words in NLTK:\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'is', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = stopwords.words('english')\r\n",
    "all_stopwords.remove('is')\r\n",
    "\r\n",
    "text_tokens = word_tokenize(example_sentence)\r\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe SpaCy library in Python is yet another extremely useful language for natural language processing in Python.\\n\\nTo install SpaCy, you have to execute the following script on your command terminal:\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Using the SpaCy Library\r\n",
    "'''\r\n",
    "The SpaCy library in Python is yet another extremely useful language for natural language processing in Python.\r\n",
    "\r\n",
    "To install SpaCy, you have to execute the following script on your command terminal:\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/thinc/compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/h5py/__init__.py\", line 25, in <module>\n",
      "    from . import _errors\n",
      "  File \"h5py/_errors.pyx\", line 1, in init h5py._errors\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "Once the library is downloaded, you also need to download the language model.\r\n",
    "Several models exist in SpaCy for different languages.\r\n",
    "We will be installing the English language model. Execute the following command in your terminal:\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/premkumargontrand/nlp/nlpvenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "sp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "all_stopwords = sp.Defaults.stop_words\r\n",
    "\r\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\r\n",
    "text_tokens = word_tokenize(text)\r\n",
    "tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding and Removing Stop Words in SpaCy Default Stop Word List\r\n",
    "'''\r\n",
    "Like the other NLP libraries, you can also add or remove stop words from the default stop word list in Spacy.\r\n",
    " But before that, we will see a list of all the existing stop words in SpaCy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n",
      "-----\n",
      "{'get', 'his', 'part', 'put', 'the', 'either', 'n‘t', 'one', 'are', 'than', 'her', 'latterly', 'must', 'cannot', 're', 'made', 'call', 'did', 'in', \"'s\", 'really', 'beyond', 'forty', 'six', 'you', 'thru', 'hence', 'serious', 'due', '‘re', 'herein', 'towards', 'full', 'empty', 'should', 'or', 'many', \"'m\", \"n't\", 'hereby', 'all', 'she', 'anyhow', 'somehow', 'across', 'again', 'from', 'more', 'mostly', 'however', 'neither', 'whereas', 'every', 'least', 'at', 'not', 'hers', 'until', 'few', 'fifteen', 'herself', 'make', 'indeed', 'five', 'over', 'bottom', 'during', 'onto', 'side', 'while', 'ten', 'our', 'those', 'doing', 'under', 'hereupon', 'before', 'thereby', 'whether', 'seeming', 'might', 'therein', 'afterwards', 'otherwise', 'they', 'among', 'whither', 'whenever', 'against', 'within', 'moreover', 'eight', 'no', 'but', 'n’t', \"'ll\", 'four', 'although', 'only', 'own', 'with', 'does', 'up', 'could', 'noone', 'your', '‘ve', 'take', 'it', 'were', 'and', 'whoever', 'about', 'when', \"'d\", 'wherein', 'even', '‘ll', 'me', 'whence', 'formerly', 'quite', 'using', '’re', 'then', 'everywhere', 'seems', 'may', 'various', 'go', 'around', 'anywhere', 'to', 'Inc', 'whereafter', 'themselves', 'through', 'yourselves', 'would', 'per', 'both', 'seemed', 'front', 'do', 'less', 'who', 'these', 'this', 'for', 'some', 'beforehand', '’d', 'former', 'here', 'how', 'whole', 'else', 'move', 'each', 'out', 'had', 'show', 'been', 'their', 'most', 'on', 'amongst', 'twelve', 'therefore', 'after', 'behind', 'regarding', 'well', 'them', 'meanwhile', 'since', 'except', 'become', 'below', 'my', 'whereupon', 'an', 'if', 'ourselves', 'thereupon', 'i', 'hereafter', 'above', 'became', '’s', 'sixty', 'several', 'ca', 'enough', 'its', 'whatever', 'further', '’ll', 'sometime', 'nine', 'by', 'itself', 'yours', 'another', 'next', \"'ve\", 'nowhere', 'down', 'am', 'say', 'which', 'fifty', 'first', 'besides', 'was', 'along', 'also', 'rather', 'others', 'very', 'back', 'nor', 'somewhere', 'there', 'is', 'once', 'just', 'himself', 'between', 'alone', 'beside', 'anyone', 'often', 'see', 'always', 'via', 'nobody', 'nothing', 'latter', 'yet', 'upon', 'because', 'anyway', 'now', 'someone', 'ours', '’m', 'can', 'such', 'last', 'will', 'elsewhere', '’ve', 'what', 'much', 'namely', 'off', 'without', 'perhaps', 'into', 'done', 'though', '‘s', 'same', 'being', 'becoming', 'nevertheless', 'has', 'third', 'seem', 'too', 'everyone', 'name', 'unless', 'of', 'never', 'thereafter', 'sometimes', 'eleven', 'him', 'still', 'thus', 'we', 'whom', 'why', 'give', 'so', 'two', 'as', 'a', 'myself', 'everything', 'ever', 'becomes', 'amount', 'whereby', 'together', 'please', 'something', 'mine', 'that', \"'re\", '‘m', '‘d', 'already', 'hundred', 'he', 'yourself', 'other', 'keep', 'where', 'us', 'throughout', 'whose', 'twenty', 'be', 'have', 'toward', 'top', 'used', 'anything', 'none', 'thence', 'wherever', 'almost', 'any', 'three'}\n"
     ]
    }
   ],
   "source": [
    "print(len(all_stopwords))\r\n",
    "print('-----')\r\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "Adding Stop Words to Default SpaCy Stop Words List\r\n",
    "The SpaCy stop word list is basically a set of strings. You can add a new word to the set like you would add any new item to a set.\r\n",
    "\r\n",
    "Look at the following script in which we add the word \"AI\" to existing list of stop words in Spacy:\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "sp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "all_stopwords = sp.Defaults.stop_words\r\n",
    "all_stopwords.add(\"Inc\")\r\n",
    "\r\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\r\n",
    "\r\n",
    "text_tokens = word_tokenize(text)\r\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'get', 'his', 'part', 'put', 'the', 'either', 'n‘t', 'one', 'are', 'than', 'her', 'latterly', 'must', 'cannot', 're', 'made', 'call', 'did', 'in', \"'s\", 'really', 'beyond', 'forty', 'six', 'you', 'thru', 'hence', 'serious', 'due', '‘re', 'herein', 'towards', 'full', 'empty', 'should', 'or', 'many', \"'m\", \"n't\", 'hereby', 'all', 'she', 'anyhow', 'somehow', 'across', 'again', 'from', 'more', 'mostly', 'however', 'neither', 'whereas', 'every', 'least', 'at', 'not', 'hers', 'until', 'few', 'fifteen', 'herself', 'make', 'indeed', 'five', 'over', 'bottom', 'during', 'onto', 'side', 'while', 'ten', 'our', 'those', 'doing', 'under', 'hereupon', 'before', 'thereby', 'whether', 'seeming', 'might', 'therein', 'afterwards', 'otherwise', 'they', 'among', 'whither', 'whenever', 'against', 'within', 'moreover', 'eight', 'no', 'but', 'n’t', \"'ll\", 'four', 'although', 'only', 'own', 'with', 'does', 'up', 'could', 'noone', 'your', '‘ve', 'take', 'it', 'were', 'and', 'whoever', 'about', 'when', \"'d\", 'wherein', 'even', '‘ll', 'me', 'whence', 'formerly', 'quite', 'using', '’re', 'then', 'everywhere', 'seems', 'may', 'various', 'go', 'around', 'anywhere', 'to', 'Inc', 'whereafter', 'themselves', 'through', 'yourselves', 'would', 'per', 'both', 'seemed', 'front', 'do', 'less', 'who', 'these', 'this', 'for', 'some', 'beforehand', '’d', 'former', 'here', 'how', 'whole', 'else', 'move', 'each', 'out', 'had', 'show', 'been', 'their', 'most', 'on', 'amongst', 'twelve', 'therefore', 'after', 'behind', 'regarding', 'well', 'them', 'meanwhile', 'since', 'except', 'become', 'below', 'my', 'whereupon', 'an', 'if', 'ourselves', 'thereupon', 'i', 'hereafter', 'above', 'became', '’s', 'sixty', 'several', 'ca', 'enough', 'its', 'whatever', 'further', '’ll', 'sometime', 'nine', 'by', 'itself', 'yours', 'another', 'next', \"'ve\", 'nowhere', 'down', 'am', 'say', 'which', 'fifty', 'first', 'besides', 'was', 'along', 'also', 'rather', 'others', 'very', 'back', 'nor', 'somewhere', 'there', 'is', 'once', 'just', 'himself', 'between', 'alone', 'beside', 'anyone', 'often', 'see', 'always', 'via', 'nobody', 'nothing', 'latter', 'yet', 'upon', 'because', 'anyway', 'now', 'someone', 'ours', '’m', 'can', 'such', 'last', 'will', 'elsewhere', '’ve', 'what', 'much', 'namely', 'off', 'without', 'perhaps', 'into', 'done', 'though', '‘s', 'same', 'being', 'becoming', 'nevertheless', 'has', 'third', 'seem', 'too', 'everyone', 'name', 'unless', 'of', 'never', 'thereafter', 'sometimes', 'eleven', 'him', 'still', 'thus', 'we', 'whom', 'why', 'give', 'so', 'two', 'as', 'a', 'myself', 'everything', 'ever', 'becomes', 'amount', 'whereby', 'together', 'please', 'something', 'mine', 'that', \"'re\", '‘m', '‘d', 'already', 'hundred', 'he', 'yourself', 'other', 'keep', 'where', 'us', 'throughout', 'whose', 'twenty', 'be', 'have', 'toward', 'top', 'used', 'anything', 'none', 'thence', 'wherever', 'almost', 'any', 'three'}\n"
     ]
    }
   ],
   "source": [
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "Removing Stop Words from Default SpaCy Stop Words List\r\n",
    "To remove a word from the set of stop words in SpaCy, you can pass the word to remove to the remove method of the set.\r\n",
    "\r\n",
    "The following script removes the word \"is\" from the set of stop words in SpaCy:\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\r\n",
    "sp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "all_stopwords = sp.Defaults.stop_words\r\n",
    "all_stopwords.add(\"is\")\r\n",
    "\r\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\r\n",
    "\r\n",
    "text_tokens = word_tokenize(text)\r\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\r\n",
    "\r\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "Using Custom Script to Remove Stop Words\r\n",
    "If you want full control over stop word removal, you can write your own script to remove stop words from your string.\r\n",
    "\r\n",
    "The first step in this regard is to define a list of words that you want treated as stop words.\r\n",
    "Let's create a list of some of the most commonly used stop words:\r\n",
    "\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \\\r\n",
    "     \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves'\\\r\n",
    "     , 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \\\r\n",
    "     'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \\\r\n",
    "     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \\\r\n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \\\r\n",
    "     'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', \\\r\n",
    "      'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\\\r\n",
    "      'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \\\r\n",
    "      'about', 'against', 'between', 'into', 'through', \\\r\n",
    "      'during', 'before', 'after', 'above', 'below',\\\r\n",
    "      'to', 'from', 'up', 'down', 'in', 'out', \\\r\n",
    "      'on', 'off', 'over', 'under', 'again',\\\r\n",
    "      'further', 'then', 'once', 'here', 'there',\\\r\n",
    "      'when', 'where', 'why', 'how', 'all', 'any',\\\r\n",
    "      'both', 'each', 'few', 'more', 'most', 'other',\\\r\n",
    "      'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\\\r\n",
    "      'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \\\r\n",
    "      \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\\\r\n",
    "     \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \\\r\n",
    "     'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', \\\r\n",
    "     'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \\\r\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1182519169.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtokens_filtered= [word for word itokensns if not word in my_stopwords]\u001b[39m\n                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "def remove_mystopwords(sentence):\r\n",
    "    tokens = sentence.split(\" \")\r\n",
    "    tokens_filtered= [word for word itokensns if not word in my_stopwords]\r\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mFacebook is an American online social media and social networking service owned by Facebook, Inc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m filtered_text = \u001b[43mremove_mystopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(filtered_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mremove_mystopwords\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremove_mystopwords\u001b[39m(sentence):\n\u001b[32m      2\u001b[39m     tokens = sentence.split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     tokens_filtered= [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext_tokens\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m my_stopwords]\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m).join(tokens_filtered)\n",
      "\u001b[31mNameError\u001b[39m: name 'text_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\r\n",
    "filtered_text = remove_mystopwords(text)\r\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "135e78ef6267b613ce7b86630936d470174b66187aad9f784a45e5cc3235687c"
  },
  "kernelspec": {
   "display_name": "nlpvenv",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
