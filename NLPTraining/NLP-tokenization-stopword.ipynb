{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57539bf-bd97-4c23-9929-a20bbc2e31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9174668d-d0b5-464f-bc70-080bc2b1723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae4f617-0f3b-4b7b-b283-8a6c4b47c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3596cd89-9f16-43be-8490-52167f362ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfbdb1d-a161-49fa-bd16-deb9b5edeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"First Sentence. Second Sentence. Mr. Third's Sentence. Who's Mr. Four?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37165a77-e06c-4223-a81b-c42b9869c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Sentence.', 'Second Sentence.', \"Mr. Third's Sentence.\", \"Who's Mr. Four?\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df25e7d-8558-4edd-86c5-12ee3becb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence', '.', 'Second', 'Sentence', '.', 'Mr.', 'Third', \"'s\", 'Sentence', '.', 'Who', \"'s\", 'Mr.', 'Four', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6170606-3f9d-4753-984d-7d0cec3c0852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence', 'Second', 'Sentence', 'Mr', 'Third', 's', 'Sentence', 'Who', 's', 'Mr', 'Four']\n"
     ]
    }
   ],
   "source": [
    "#tokenizer= RegexpTokenizer(r'\\w+|$[0-9]+|\\S+')\n",
    "tokenizer= RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3634d9-136b-4ceb-b9c7-e2932da5fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.,', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "sent0 = \"\"\"Thomas Jefferson began building Monticello at the age of 26., thanks\"\"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9]+|\\S+')\n",
    "print(tokenizer.tokenize(sent0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c41d9a-bf6e-421f-9c63-5c948637e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a607c9f-2359-492a-b970-55b17a47117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence', '.', 'Second', 'Sentence', '.', 'Mr', '.', 'Third', \"'\", 's', 'Sentence', '.', 'Who', \"'\", 's', 'Mr', '.', 'Four', '?']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b123594-b3e8-468d-b171-c44e1ec813dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Sentence.', 'Second', 'Sentence.', 'Mr.', 'Third', \"'s\", 'Sentence.', 'Who', \"'s\", 'Mr.', 'Four', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e813d-05a4-4421-9865-9e739ac9a904",
   "metadata": {},
   "source": [
    "# Stop word removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483e4c2c-994f-4fe0-a76f-84d83ca3122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea367423-e6a2-4352-bdc2-524f134a68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3302b2cf-69b9-4ab0-ad8b-612450cfb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94494262-025f-43ee-b149-61807b94f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 'no', \"didn't\", 'them', 'haven', 'needn', 'all', 'does', 'ain', 'wasn', 'being', \"i've\", 'own', 'over', 'because', 's', 'so', 'why', 'under', \"doesn't\", 'out', 'those', 'if', 'me', 'further', 'ourselves', 'more', 'will', 'itself', \"they're\", 'which', 'as', 'isn', 'yourselves', \"you've\", 'most', 'ma', 'was', 'll', 'up', \"she'd\", 'the', 'from', 'then', 'wouldn', \"should've\", 'its', 'just', 't', 'such', 'with', \"weren't\", \"it's\", 'be', 'how', 'weren', 'whom', 'shan', 'you', 'by', 'while', 'again', \"don't\", 'these', 'mustn', 'my', 'themselves', 'hasn', \"he'd\", 'myself', \"you'd\", 'herself', 'couldn', 'above', 'don', 'yourself', 'do', 'mightn', 'too', 'can', 'when', 'into', 'him', \"i'm\", 'is', 'on', \"we've\", 'in', 'are', 'there', 'y', 'did', 'at', 'through', 'any', 'doing', 'it', 'same', 'before', 'didn', 'his', \"we'd\", 'once', 'some', 'shouldn', 'not', 'below', 'but', 'has', 'their', 'for', 'hers', 've', \"shan't\", 'after', 'both', 'your', 'of', 'aren', 'few', 'doesn', \"isn't\", 'only', 'who', \"she's\", 'had', 'that', \"wasn't\", 'we', 'to', 'her', 'what', 'where', \"needn't\", \"we're\", 'they', \"couldn't\", 're', 'very', 'ours', 'were', 'other', \"haven't\", \"he'll\", \"wouldn't\", 'won', \"we'll\", 'i', 'now', 'o', 'nor', \"it'd\", 'until', 'yours', 'a', 'been', 'during', 'this', 'have', 'm', \"they've\", 'and', 'our', 'down', 'theirs', \"they'd\", \"hasn't\", 'am', 'an', 'hadn', 'against', \"it'll\", \"they'll\", \"won't\", 'he', \"aren't\", 'each', \"she'll\", 'himself', \"he's\", \"shouldn't\", \"mustn't\", \"you're\", \"i'd\", 'she', 'than', 'between', 'having', 'about', 'or', \"you'll\", \"i'll\", \"mightn't\", \"that'll\", \"hadn't\", 'here', 'should', 'd'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bd3362e-16b3-470f-a3f8-589975b89e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'is', 'an', 'American', 'online', 'social', 'media', 'and', 'social', 'networking', 'service', 'owned', 'by', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e306c4eb-4660-4eee-85eb-5f5558c46aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "#\\/| Mr, Mrs. @  - \n",
    "filtered_sentence = []\n",
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a9b18f4-7669-41f2-b300-9bd6f68c9720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('online')\n",
    "\n",
    "text_tokens = word_tokenize(example_sentence)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6c8c05-867e-4718-bbc5-1334e85d82e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'media', 'networking', 'service', 'owned', 'Facebook', ',', 'Inc', '.']\n"
     ]
    }
   ],
   "source": [
    "sw_list = ['social','online']\n",
    "all_stopwords.extend(sw_list)\n",
    "\n",
    "text_tokens = word_tokenize(example_sentence)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dd84d01-6d62-47f7-8468-238dc6834406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/03/2a/43afac516eb82409ca47d7206f982beaf265d2ba06a72ca07cf06b290c20/spacy-3.8.7-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached spacy-3.8.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/14/b0/3ee762e98cf9a8c2df9c8b377c326f3dd4495066d4eace9066fca46eba7a/murmurhash-1.0.13-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached murmurhash-1.0.13-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/41/b4/7546faf2ab63e59befc95972316d62276cec153f7d4d60e7b0d5e08f0602/cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/e0/27/0fd36b63caa8bbf57b31a121d9565d385bbd7521771d4eb93e17d326873d/preshed-3.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached preshed-3.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.4.0,>=8.3.4 from https://files.pythonhosted.org/packages/f0/53/5f9eeb725c2ca94adef76a2cd0289bc530728b0a035eed815c766a9291ef/thinc-8.3.6-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached thinc-8.3.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/06/7c/34330a89da55610daa5f245ddce5aab81244321101614751e7537f125133/wasabi-1.1.3-py3-none-any.whl.metadata\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/41/47/1bdaad84502df973ecb8ca658117234cf7fb20e1dec60da71dce82de993f/srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.5.0,>=0.1.0 from https://files.pythonhosted.org/packages/2a/87/abd57374044e1f627f0a905ac33c1a7daab35a3a815abfea4e1bafd3fdb1/weasel-0.4.1-py3-none-any.whl.metadata\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer<1.0.0,>=0.3.0 from https://files.pythonhosted.org/packages/1e/fa/6473c00b5eb26a2ba427813107699d3e6f4e1a4afad3f7494b17bdef3422/typer-0.19.1-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/3e/d3/108f2006987c58e76691d5ae5d200dd3e0f532cb4e5fa3560751c3a1feba/pydantic-2.11.9-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/c3/6b/068c2ea7a712bf805c62445bd9e9c06d7340358ef2824150eceac027444b/langcodes-3.5.0-py3-none-any.whl.metadata\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for language-data>=1.2 from https://files.pythonhosted.org/packages/5d/e9/5a5ffd9b286db82be70d677d0a91e4d58f7912bb8dd026ddeeb4abe70679/language_data-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/24/2f/3cfa7244ae292dd850989f328722d2aef313f74ffc471184dc509e1e4e5a/pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for blis<1.4.0,>=1.3.0 from https://files.pythonhosted.org/packages/c1/13/a3b66fd57c75343a5b2e6323cd8f73bdd2e9b328deba7cf676ec334ec754/blis-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached blis-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for shellingham>=1.3.0 from https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl.metadata\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for rich>=10.11.0 from https://files.pythonhosted.org/packages/e3/30/3c4d035596d3cf444529e0b2953ad0466f6049528a879d27534700580395/rich-14.1.0-py3-none-any.whl.metadata\n",
      "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<1.0.0,>=0.7.0 from https://files.pythonhosted.org/packages/f1/72/e8e53d8232e801e040f4b557ff3a453cecbb630d53ae107bd5e66a206bb9/cloudpathlib-0.22.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for smart-open<8.0.0,>=5.2.1 from https://files.pythonhosted.org/packages/e5/d9/460cf1d58945dd771c228c29d5664f431dfc4060d3d092fed40546b11472/smart_open-7.3.1-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Obtaining dependency information for marisa-trie>=1.1.0 from https://files.pythonhosted.org/packages/a9/5a/de7936d58ed0de847180cee2b95143d420223c5ade0c093d55113f628237/marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/94/54/e7d793b573f298e1c9013b8c4dade17d481164aa517d1d7148619c2cedbf/markdown_it_py-4.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/premkumargontrand/pythonlearning082025/AILearningSep/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/31/25/3e8cc2c46b5329c5957cec959cb76a10718e1a513309c31399a4dad07eb3/wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.8.7-cp311-cp311-macosx_11_0_arm64.whl (6.4 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.13-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached preshed-3.0.10-cp311-cp311-macosx_11_0_arm64.whl (127 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp311-cp311-macosx_11_0_arm64.whl (634 kB)\n",
      "Using cached thinc-8.3.6-cp311-cp311-macosx_11_0_arm64.whl (845 kB)\n",
      "Downloading typer-0.19.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.3.0-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (158 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.9 pydantic-core-2.33.2 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.19.1 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0f27b7-6d70-4ff1-b245-fe5908ac4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29f8337d-49fd-4481-9806-44f00ff62bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08cecbbc-2627-4717-9489-b5cbdb6ec446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Facebook is an American online social media and social networking service owned by Facebook, Inc.\n",
      "without stopword: ['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "print(\"original:\", text)\n",
    "print(\"without stopword:\", tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ab789e6-85a3-419a-903a-6e2497be874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', 'American', 'online', 'social', 'media', 'social', 'networking', 'service', 'owned', 'Facebook', ',', '.']\n",
      "Facebook      facebook    \n",
      "American      american    \n",
      "online        online      \n",
      "social        social      \n",
      "media         medium      \n",
      "social        social      \n",
      "networking    network     \n",
      "service       service     \n",
      "owned         own         \n",
      "Facebook      facebook    \n",
      ",             ,           \n",
      ".             .           \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords.add(\"Inc\")\n",
    "\n",
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\n",
    "\n",
    "text_tokens = word_tokenize(text)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "\n",
    "for word in tokens_without_sw:\n",
    "    token = sp(word)[0]   # re-analyze each string\n",
    "    print(f\"{token.text:<12}  {token.lemma_:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0eb3e8cf-1a1c-4ba5-8b71-4ddba6474207",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \\\n",
    "     \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves'\\\n",
    "     , 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \\\n",
    "     'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \\\n",
    "     'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \\\n",
    "     'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \\\n",
    "     'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', \\\n",
    "      'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\\\n",
    "      'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \\\n",
    "      'about', 'against', 'between', 'into', 'through', \\\n",
    "      'during', 'before', 'after', 'above', 'below',\\\n",
    "      'to', 'from', 'up', 'down', 'in', 'out', \\\n",
    "      'on', 'off', 'over', 'under', 'again',\\\n",
    "      'further', 'then', 'once', 'here', 'there',\\\n",
    "      'when', 'where', 'why', 'how', 'all', 'any',\\\n",
    "      'both', 'each', 'few', 'more', 'most', 'other',\\\n",
    "      'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\\\n",
    "      'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \\\n",
    "      \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\\\n",
    "     \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \\\n",
    "     'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', \\\n",
    "     'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \\\n",
    "    'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d266368-c38b-4bab-ad77-7bbb37844bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mystopwords(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= [word for word in tokens if not word in my_stopwords]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bb88aff-dab9-46bb-a261-6216878c22fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook American online social media social networking service owned Facebook, Inc.\n"
     ]
    }
   ],
   "source": [
    "text = \"Facebook is an American online social media and social networking service owned by Facebook, Inc.\"\n",
    "filtered_text = remove_mystopwords(text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61f5ec-6326-42c7-b267-5912935ec551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AILearningSep",
   "language": "python",
   "name": "ailearningsep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
